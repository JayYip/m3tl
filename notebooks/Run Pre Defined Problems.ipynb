{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Copy of Run Pre-defined problems.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlENudpFZVhA"
      },
      "source": [
        "# Run Pre-defined Problems\n",
        "\n",
        "This notebook is run on Google Colab. You need to manually upload the data to kaggle and pass the correct path to the pre-processing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD36RrxixyL0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7d1bf2-58dd-4dc1-e034-3a0dbe8b66e4"
      },
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install tensorflow-addons==0.11.2\n",
        "!pip install bert-multitask-learning==0.5.7b8\n",
        "!pip install transformers==3.5.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.10.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.33.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.35.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (50.3.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNDPcXgwxoRR",
        "trusted": true
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, "../")\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from bert_multitask_learning import train_bert_multitask, train_eval_input_fn, BertMultiTask, DynamicBatchSizeParams\n",
        "from bert_multitask_learning.predefined_problems import get_weibo_ner_fn, get_weibo_cws_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MkdL8_JzZVhB"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dajzr_nYxoRV",
        "trusted": true
      },
      "source": [
        "problem_type_dict = {\n",
        "    'weibo_cws': 'seq_tag',\n",
        "    'weibo_ner': 'seq_tag'\n",
        "}\n",
        "\n",
        "# data \n",
        "processing_fn_dict = {\n",
        "    'weibo_ner': get_weibo_ner_fn(file_path='../data/ner/weiboNER_2nd_conll*'),\n",
        "    'weibo_cws': get_weibo_cws_fn(file_path='../data/ner/weiboNER_2nd_conll*')\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA5GLmYxxoRX"
      },
      "source": [
        "## Train Models\n",
        "If you don't want to control every thing, you can just call `train_bert_multitask` function. Please note that starting from 0.4.2, transformer models (the body model) are implemented using [huggingface transformers](https://github.com/huggingface/transformers) and because of that, now we can basically use all transformer models by specifying following params(below is the default value):\n",
        "\n",
        "    params.transformer_model_name = 'bert-base-chinese'\n",
        "    params.transformer_tokenizer_name = 'bert-base-chinese'\n",
        "    params.transformer_config_name = 'bert-base-chinese'\n",
        "    params.transformer_model_loading = 'TFAutoModel'\n",
        "    params.transformer_config_loading = 'BertConfig'\n",
        "    params.transformer_tokenizer_loading = 'AutoTokenizer'\n",
        "\n",
        "  And for decoder:\n",
        "\n",
        "    params.transformer_decoder_model_name = None\n",
        "    params.transformer_decoder_config_name = None\n",
        "    params.transformer_decoder_tokenizer_name = None\n",
        "    params.transformer_decoder_model_loading = 'TFAutoModel'\n",
        "    params.transformer_decoder_config_loading = 'BertConfig'\n",
        "    params.transformer_decoder_tokenizer_loading = 'AutoTokenizer'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiYKa3NhxoRY",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9d816c-4b6f-4c3e-9e74-fdf7ce571ad4"
      },
      "source": [
        "# here we use the default model which is bert-base-chinese\n",
        "params = DynamicBatchSizeParams()\n",
        "# AutoConfig cannot load from dict...\n",
        "params.transformer_config_loading = 'BertConfig'\n",
        "params.transformer_model_name = 'bert-base-chinese'\n",
        "params.transformer_tokenizer_name = 'bert-base-chinese'\n",
        "params.transformer_tokenizer_loading = 'BertTokenizer'\n",
        "train_bert_multitask(problem='weibo_ner&weibo_cws', params=params, problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict, num_gpus=1, num_epochs=10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:bert_config not exists. will load model from huggingface checkpoint.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding new problem weibo_cws, problem type: seq_tag\n",
            "Adding new problem weibo_ner, problem type: seq_tag\n",
            "INFO:tensorflow:sampling weights: \n",
            "INFO:tensorflow:weibo_cws_weibo_ner: 1.0\n",
            "INFO:tensorflow:sampling weights: \n",
            "INFO:tensorflow:weibo_cws_weibo_ner: 1.0\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initial lr: 2e-05\n",
            "INFO:tensorflow:Train steps: 80\n",
            "INFO:tensorflow:Warmup steps: 8\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "1/8 [==>...........................] - ETA: 2s - mean_acc: 0.1691 - weibo_cws_acc: 0.1864 - weibo_ner_acc: 0.3209 - weibo_ner_loss: 2.2998 - weibo_cws_loss: 1.7480WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "2/8 [======>.......................] - ETA: 3s - mean_acc: 0.2010 - weibo_cws_acc: 0.1984 - weibo_ner_acc: 0.3314 - weibo_ner_loss: 2.0489 - weibo_cws_loss: 1.8042WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3008s vs `on_train_batch_end` time: 0.6144s). Check your callbacks.\n",
            "8/8 [==============================] - 15s 2s/step - mean_acc: 0.3172 - weibo_cws_acc: 0.3587 - weibo_ner_acc: 0.5787 - weibo_ner_loss: 1.3611 - weibo_cws_loss: 1.4349 - val_loss: 1.5671 - val_mean_acc: 0.7840 - val_weibo_cws_acc: 0.6341 - val_weibo_ner_acc: 0.9241\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 25s 3s/step - mean_acc: 0.7031 - weibo_cws_acc: 0.5982 - weibo_ner_acc: 0.9211 - weibo_ner_loss: 0.4779 - weibo_cws_loss: 0.8530 - val_loss: 1.1315 - val_mean_acc: 0.7806 - val_weibo_cws_acc: 0.6282 - val_weibo_ner_acc: 0.9234\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 23s 3s/step - mean_acc: 0.6978 - weibo_cws_acc: 0.6051 - weibo_ner_acc: 0.9311 - weibo_ner_loss: 0.2667 - weibo_cws_loss: 0.7419 - val_loss: 1.0254 - val_mean_acc: 0.7863 - val_weibo_cws_acc: 0.6313 - val_weibo_ner_acc: 0.9339\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 31s 4s/step - mean_acc: 0.7142 - weibo_cws_acc: 0.6223 - weibo_ner_acc: 0.9434 - weibo_ner_loss: 0.2294 - weibo_cws_loss: 0.6793 - val_loss: 0.9181 - val_mean_acc: 0.8003 - val_weibo_cws_acc: 0.6532 - val_weibo_ner_acc: 0.9416\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 28s 4s/step - mean_acc: 0.7234 - weibo_cws_acc: 0.6408 - weibo_ner_acc: 0.9483 - weibo_ner_loss: 0.2028 - weibo_cws_loss: 0.6428 - val_loss: 0.8705 - val_mean_acc: 0.8043 - val_weibo_cws_acc: 0.6610 - val_weibo_ner_acc: 0.9416\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 25s 3s/step - mean_acc: 0.7259 - weibo_cws_acc: 0.6483 - weibo_ner_acc: 0.9483 - weibo_ner_loss: 0.2137 - weibo_cws_loss: 0.6350 - val_loss: 0.8549 - val_mean_acc: 0.8072 - val_weibo_cws_acc: 0.6667 - val_weibo_ner_acc: 0.9416\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 26s 3s/step - mean_acc: 0.7348 - weibo_cws_acc: 0.6579 - weibo_ner_acc: 0.9483 - weibo_ner_loss: 0.2563 - weibo_cws_loss: 0.6173 - val_loss: 0.8348 - val_mean_acc: 0.8128 - val_weibo_cws_acc: 0.6766 - val_weibo_ner_acc: 0.9416\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 31s 4s/step - mean_acc: 0.7316 - weibo_cws_acc: 0.6623 - weibo_ner_acc: 0.9484 - weibo_ner_loss: 0.2114 - weibo_cws_loss: 0.6141 - val_loss: 0.8229 - val_mean_acc: 0.8142 - val_weibo_cws_acc: 0.6800 - val_weibo_ner_acc: 0.9416\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 27s 3s/step - mean_acc: 0.7357 - weibo_cws_acc: 0.6665 - weibo_ner_acc: 0.9485 - weibo_ner_loss: 0.1990 - weibo_cws_loss: 0.5833 - val_loss: 0.8218 - val_mean_acc: 0.8174 - val_weibo_cws_acc: 0.6871 - val_weibo_ner_acc: 0.9416\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 24s 3s/step - mean_acc: 0.7393 - weibo_cws_acc: 0.6692 - weibo_ner_acc: 0.9485 - weibo_ner_loss: 0.1962 - weibo_cws_loss: 0.5869 - val_loss: 0.8216 - val_mean_acc: 0.8182 - val_weibo_cws_acc: 0.6890 - val_weibo_ner_acc: 0.9416\n",
            "Model: \"BertMultiTask\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "BertMultiTaskBody (BertMulti multiple                  102267648 \n",
            "_________________________________________________________________\n",
            "BertMultiTaskTop (BertMultiT multiple                  10770     \n",
            "_________________________________________________________________\n",
            "mean_acc (Mean)              multiple                  2         \n",
            "=================================================================\n",
            "Total params: 102,278,420\n",
            "Trainable params: 102,278,414\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bert_multitask_learning.model_fn.BertMultiTask at 0x7fe707e5e128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD9AxZrpxoRa"
      },
      "source": [
        "If you want to take more control of the training process, you can use lower level api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MckZxWKjxoRb",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352152af-14be-430c-feca-643ed81c309c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from bert_multitask_learning import train_eval_input_fn\n",
        "\n",
        "problem = 'weibo_ner&weibo_cws'\n",
        "num_gpus = 1\n",
        "bert_multitask_params = DynamicBatchSizeParams()\n",
        "bert_multitask_params.transformer_config_loading = 'BertConfig'\n",
        "bert_multitask_params.transformer_model_name = 'bert-base-chinese'\n",
        "bert_multitask_params.transformer_tokenizer_name = 'bert-base-chinese'\n",
        "bert_multitask_params.transformer_tokenizer_loading = 'BertTokenizer'\n",
        "\n",
        "bert_multitask_params.add_multiple_problems(\n",
        "    problem_type_dict=problem_type_dict, processing_fn_dict=processing_fn_dict)\n",
        "\n",
        "# assign problem to params\n",
        "bert_multitask_params.train_epoch = 1\n",
        "bert_multitask_params.assign_problem(problem, gpu=1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:bert_config not exists. will load model from huggingface checkpoint.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding new problem weibo_cws, problem type: seq_tag\n",
            "Adding new problem weibo_ner, problem type: seq_tag\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpiKdg1fxoRd",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5572a95c-2c0b-43a2-b1d1-d626d72112ae"
      },
      "source": [
        "\n",
        "dist_trategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "# create dataset\n",
        "train_dataset = train_eval_input_fn(bert_multitask_params)\n",
        "eval_dataset = train_eval_input_fn(bert_multitask_params, mode=tf.estimator.ModeKeys.EVAL)\n",
        "\n",
        "train_dataset = dist_trategy.experimental_distribute_dataset(\n",
        "    train_dataset)\n",
        "eval_dataset = dist_trategy.experimental_distribute_dataset(\n",
        "    eval_dataset)\n",
        "\n",
        "# create model\n",
        "with dist_trategy.scope():\n",
        "    model = BertMultiTask(params=bert_multitask_params)\n",
        "    model.compile()\n",
        "    model.fit(\n",
        "        x=train_dataset,\n",
        "        validation_data=eval_dataset,\n",
        "        epochs=1,\n",
        "        steps_per_epoch=8,\n",
        "        validation_steps=1\n",
        "    )\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:sampling weights: \n",
            "INFO:tensorflow:weibo_cws_weibo_ner: 1.0\n",
            "INFO:tensorflow:sampling weights: \n",
            "INFO:tensorflow:weibo_cws_weibo_ner: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initial lr: 2e-05\n",
            "INFO:tensorflow:Train steps: 84\n",
            "INFO:tensorflow:Warmup steps: 8\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "8/8 [==============================] - 8s 978ms/step - mean_acc: 0.3168 - weibo_cws_acc: 0.4526 - weibo_ner_acc: 0.4622 - weibo_ner_loss: 1.3803 - weibo_cws_loss: 1.2806 - val_loss: 1.3114 - val_mean_acc: 0.7887 - val_weibo_cws_acc: 0.6315 - val_weibo_ner_acc: 0.9459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufZk0jatfOPA"
      },
      "source": [
        "# the saved model contains variables of optimizers which are not needed when doing prediction\n",
        "# we can trim the model size by removing those variables\n",
        "from bert_multitask_learning import trim_checkpoint_for_prediction\n",
        "trim_checkpoint_for_prediction(problem=problem, input_dir='./models/weibo_cws_weibo_ner_ckpt', output_dir='./models/trimmed_ckpt', overwrite=True, problem_type_dict=problem_type_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWitn_Tvf8A9"
      },
      "source": [
        "! du -sh ./models/weibo_cws_weibo_ner_ckpt\n",
        "! du -sh ./models/trimmed_ckpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUQ_418MxoRi"
      },
      "source": [
        "## Evaluate and Predict\n",
        "\n",
        "~~For NER and CWS, we need different evaluation logic.~~ Evaluation has bug and not fixed now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ScABR3xoRi",
        "trusted": true
      },
      "source": [
        "from bert_multitask_learning import predict_bert_multitask"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaW8KwkDxoRp",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc9df50-b30c-4b1a-af2a-2cdc02950338"
      },
      "source": [
        "# predict\n",
        "import numpy as np\n",
        "from bert_multitask_learning.utils import get_or_make_label_encoder\n",
        "# get prediction generator\n",
        "pred_prob, model = predict_bert_multitask(\n",
        "    inputs=['中国和美国在打贸易战']*10, \n",
        "    problem='weibo_cws&weibo_ner', \n",
        "    processing_fn_dict=processing_fn_dict,\n",
        "    problem_type_dict=problem_type_dict,\n",
        "    model_dir='./models/trimmed_ckpt',\n",
        "    return_model=True)\n",
        "predict_params = model.params\n",
        "# get label encoder\n",
        "ner_label_encoder = get_or_make_label_encoder(params=predict_params, problem='weibo_ner', mode='predict', label_list=[])\n",
        "cws_label_encoder = get_or_make_label_encoder(params=predict_params, problem='weibo_cws', mode='predict', label_list=[])\n",
        "\n",
        "for problem_name, prob in pred_prob.items():\n",
        "    ner_pred = np.argmax(prob, axis = -1)\n",
        "    print(ner_label_encoder.inverse_transform(ner_pred[0].tolist()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding new problem weibo_cws, problem type: seq_tag\n",
            "Adding new problem weibo_ner, problem type: seq_tag\n",
            "INFO:tensorflow:Checkpoint dir: ./models/trimmed_ckpt\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:tensorflow:中国和美国在打贸易战\n",
            "INFO:tensorflow:input_ids: [101, 704, 1744, 1469, 5401, 1744, 1762, 2802, 6588, 3211, 2773, 102]\n",
            "INFO:tensorflow:input_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "INFO:tensorflow:segment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['B-GPE' 'B-GPE' 'B-GPE' 'B-GPE' 'B-GPE' 'B-GPE' 'B-GPE' 'B-GPE' 'B-GPE'\n",
            " 'B-LOC' 'B-LOC' 'B-PER']\n",
            "['B-GPE' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' '[PAD]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr9RwRKQgdSS"
      },
      "source": [
        "# you can also make prediction using model directly\n",
        "from bert_multitask_learning import predict_input_fn\n",
        "predict_dataset = predict_input_fn(['中国和美国在打贸易战']*10, params)\n",
        "pred_prob = model.predict(predict_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkvQq9pxg26W"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}
