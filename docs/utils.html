---

title: Utils


keywords: fastai
sidebar: home_sidebar



nb_path: "source_nbs/01_utils.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: source_nbs/01_utils.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;-1&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="load_transformer_tokenizer" class="doc_header"><code>load_transformer_tokenizer</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L25" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>load_transformer_tokenizer</code>(<strong><code>tokenizer_name</code></strong>:<code>str</code>, <strong><code>load_module_name</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>some tokenizers cannot be loaded using AutoTokenizer.</p>
<p>this function served as a util function to catch that situation.</p>
<p>Args:
    tokenizer_name (str): tokenizer name</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">load_transformer_tokenizer</span><span class="p">(</span>
            <span class="s1">&#39;voidful/albert_chinese_tiny&#39;</span><span class="p">,</span> <span class="s1">&#39;BertTokenizer&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>PreTrainedTokenizer(name_or_path=&#39;voidful/albert_chinese_tiny&#39;, vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side=&#39;right&#39;, special_tokens={&#39;unk_token&#39;: &#39;[UNK]&#39;, &#39;sep_token&#39;: &#39;[SEP]&#39;, &#39;pad_token&#39;: &#39;[PAD]&#39;, &#39;cls_token&#39;: &#39;[CLS]&#39;, &#39;mask_token&#39;: &#39;[MASK]&#39;})</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="load_transformer_config" class="doc_header"><code>load_transformer_config</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L42" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>load_transformer_config</code>(<strong><code>config_name_or_dict</code></strong>, <strong><code>load_module_name</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Some models need specify loading module</p>
<p>Args:
    config_name (str): module name
    load_module_name (str, optional): loading module name. Defaults to None.</p>
<p>Returns:
    config: config</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">load_transformer_config</span><span class="p">(</span>
    <span class="s1">&#39;bert-base-chinese&#39;</span><span class="p">)</span>
<span class="n">config_dict</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
<span class="c1"># load config with dict</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">load_transformer_config</span><span class="p">(</span>
    <span class="n">config_dict</span><span class="p">,</span> <span class="n">load_module_name</span><span class="o">=</span><span class="s1">&#39;BertConfig&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="load_transformer_model" class="doc_header"><code>load_transformer_model</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L67" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>load_transformer_model</code>(<strong><code>model_name_or_config</code></strong>, <strong><code>load_module_name</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># this is a pt only model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_transformer_model</span><span class="p">(</span>
    <span class="s1">&#39;voidful/albert_chinese_tiny&#39;</span><span class="p">)</span>

<span class="c1"># load by config (not load weights)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_transformer_model</span><span class="p">(</span><span class="n">load_transformer_config</span><span class="p">(</span>
    <span class="s1">&#39;bert-base-chinese&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>404 Client Error: Not Found for url: https://huggingface.co/voidful/albert_chinese_tiny/resolve/main/tf_model.h5
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: [&#39;predictions.LayerNorm.weight&#39;, &#39;predictions.LayerNorm.bias&#39;, &#39;predictions.dense.bias&#39;, &#39;predictions.bias&#39;, &#39;predictions.decoder.bias&#39;, &#39;predictions.dense.weight&#39;, &#39;predictions.decoder.weight&#39;]
- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of TFAlbertModel were initialized from the PyTorch model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LabelEncoder" class="doc_header"><code>class</code> <code>LabelEncoder</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L86" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LabelEncoder</code>() :: <code>BaseEstimator</code></p>
</blockquote>
<p>Base class for all estimators in scikit-learn</p>
<h2 id="Notes">Notes<a class="anchor-link" href="#Notes"> </a></h2><p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="create_path" class="doc_header"><code>create_path</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L170" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>create_path</code>(<strong><code>path</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_or_make_label_encoder" class="doc_header"><code>get_or_make_label_encoder</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L175" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_or_make_label_encoder</code>(<strong><code>params</code></strong>, <strong><code>problem</code></strong>:<code>str</code>, <strong><code>mode</code></strong>:<code>str</code>, <strong><code>label_list</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Function to unify ways to get or create label encoder for various
problem type.</p>
<p>cls: LabelEncoder
seq_tag: LabelEncoder
multi_cls: MultiLabelBinarizer
seq2seq_text: Tokenizer</p>
<p>Arguments:
    problem {str} -- problem name
    mode {mode} -- mode</p>
<p>Keyword Arguments:
    label_list {list} -- label list to fit the encoder (default: {None})</p>
<p>Returns:
    LabelEncoder -- label encoder</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">le_train</span> <span class="o">=</span> <span class="n">get_or_make_label_encoder</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">problem</span><span class="o">=</span><span class="s1">&#39;weibo_fake_ner&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">bert_multitask_learning</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span> <span class="n">label_list</span><span class="o">=</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]]</span>
<span class="p">)</span>
<span class="c1"># seq_tag will add [PAD]</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">le_train</span><span class="o">.</span><span class="n">encode_dict</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>

<span class="n">le_predict</span> <span class="o">=</span> <span class="n">get_or_make_label_encoder</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">problem</span><span class="o">=</span><span class="s1">&#39;weibo_fake_ner&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">bert_multitask_learning</span><span class="o">.</span><span class="n">PREDICT</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">le_predict</span><span class="o">.</span><span class="n">encode_dict</span><span class="o">==</span><span class="n">le_train</span><span class="o">.</span><span class="n">encode_dict</span>

<span class="c1"># list train</span>
<span class="n">le_train</span> <span class="o">=</span> <span class="n">get_or_make_label_encoder</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">problem</span><span class="o">=</span><span class="s1">&#39;weibo_fake_cls&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">bert_multitask_learning</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span> <span class="n">label_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">]</span>
<span class="p">)</span>
<span class="c1"># seq_tag will add [PAD]</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">le_train</span><span class="o">.</span><span class="n">encode_dict</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>

<span class="n">le_predict</span> <span class="o">=</span> <span class="n">get_or_make_label_encoder</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">problem</span><span class="o">=</span><span class="s1">&#39;weibo_fake_cls&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">bert_multitask_learning</span><span class="o">.</span><span class="n">PREDICT</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">le_predict</span><span class="o">.</span><span class="n">encode_dict</span><span class="o">==</span><span class="n">le_train</span><span class="o">.</span><span class="n">encode_dict</span>

<span class="c1"># text</span>
<span class="n">le_train</span> <span class="o">=</span> <span class="n">get_or_make_label_encoder</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">problem</span><span class="o">=</span><span class="s1">&#39;weibo_masklm&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">bert_multitask_learning</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">le_train</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">PreTrainedTokenizer</span><span class="p">)</span>
<span class="n">le_predict</span> <span class="o">=</span> <span class="n">get_or_make_label_encoder</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">problem</span><span class="o">=</span><span class="s1">&#39;weibo_masklm&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">bert_multitask_learning</span><span class="o">.</span><span class="n">PREDICT</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">le_predict</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">PreTrainedTokenizer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="cluster_alphnum" class="doc_header"><code>cluster_alphnum</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L281" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>cluster_alphnum</code>(<strong><code>text</code></strong>:<code>str</code>)</p>
</blockquote>
<p>Simple funtions to aggregate eng and number</p>
<p>Arguments:
    text {str} -- input text</p>
<p>Returns:
    list -- list of string with chinese char or eng word as element</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="filter_empty" class="doc_header"><code>filter_empty</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L314" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>filter_empty</code>(<strong><code>input_list</code></strong>, <strong><code>target_list</code></strong>)</p>
</blockquote>
<p>Filter empty inputs or targets</p>
<p>Arguments:
    input_list {list} -- input list
    target_list {list} -- target list</p>
<p>Returns:
    input_list, target_list -- data after filter</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="infer_shape_and_type_from_dict" class="doc_header"><code>infer_shape_and_type_from_dict</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L335" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>infer_shape_and_type_from_dict</code>(<strong><code>inp_dict</code></strong>:<code>dict</code>, <strong><code>fix_dim_for_high_rank_tensor</code></strong>=<em><code>True</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;test1&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span>
    <span class="s1">&#39;test2&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">),</span>
    <span class="s1">&#39;test5&#39;</span><span class="p">:</span> <span class="mi">5</span>
<span class="p">}</span>
<span class="n">desc_dict</span> <span class="o">=</span> <span class="n">infer_shape_and_type_from_dict</span><span class="p">(</span>
    <span class="n">test_dict</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">desc_dict</span> <span class="o">==</span> <span class="p">({</span><span class="s1">&#39;test1&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span> <span class="s1">&#39;test2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="s1">&#39;test5&#39;</span><span class="p">:</span> <span class="p">[]},</span> <span class="p">{</span>
                    <span class="s1">&#39;test1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="s1">&#39;test2&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="s1">&#39;test5&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_transformer_main_model" class="doc_header"><code>get_transformer_main_model</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L377" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_transformer_main_model</code>(<strong><code>model</code></strong>, <strong><code>key</code></strong>=<em><code>'embeddings'</code></em>)</p>
</blockquote>
<p>Function to extrac model name from huggingface transformer models.</p>
<p>Args:
    model (Model): Huggingface transformers model
    key (str, optional): Key to identify model. Defaults to 'embeddings'.</p>
<p>Returns:
    model</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">load_transformer_model</span><span class="p">(</span>
    <span class="s1">&#39;voidful/albert_chinese_tiny&#39;</span><span class="p">)</span>
<span class="n">main_model</span> <span class="o">=</span> <span class="n">get_transformer_main_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nb">isinstance</span><span class="p">(</span><span class="n">main_model</span><span class="p">,</span> <span class="n">transformers</span><span class="o">.</span><span class="n">TFAlbertMainLayer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>404 Client Error: Not Found for url: https://huggingface.co/voidful/albert_chinese_tiny/resolve/main/tf_model.h5
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: [&#39;predictions.LayerNorm.weight&#39;, &#39;predictions.LayerNorm.bias&#39;, &#39;predictions.dense.bias&#39;, &#39;predictions.bias&#39;, &#39;predictions.decoder.bias&#39;, &#39;predictions.dense.weight&#39;, &#39;predictions.decoder.weight&#39;]
- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of TFAlbertModel were initialized from the PyTorch model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.
WARNING:tensorflow:From /data/anaconda3/lib/python3.8/inspect.py:350: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /data/anaconda3/lib/python3.8/inspect.py:350: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>True</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_embedding_table_from_model" class="doc_header"><code>get_embedding_table_from_model</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L397" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_embedding_table_from_model</code>(<strong><code>model</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embedding_table_from_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">embedding</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">21128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_shape_list" class="doc_header"><code>get_shape_list</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L405" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_shape_list</code>(<strong><code>tensor</code></strong>, <strong><code>expected_rank</code></strong>=<em><code>None</code></em>, <strong><code>name</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Returns a list of the shape of tensor, preferring static dimensions.</p>
<p>Args:
  tensor: A tf.Tensor object to find the shape of.
  expected_rank: (optional) int. The expected rank of <code>tensor</code>. If this is
    specified and the <code>tensor</code> has a different rank, and exception will be
    thrown.
  name: Optional name of the tensor for the error message.</p>
<p>Returns:
  A list of dimensions of the shape of tensor. All static dimensions will
  be returned as python integers, and dynamic dimensions will be returned
  as tf.Tensor scalars.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="gather_indexes" class="doc_header"><code>gather_indexes</code><a href="https://github.com/JayYip/bert-multitask-learning/tree/master/bert_multitask_learning/utils.py#L436" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>gather_indexes</code>(<strong><code>sequence_tensor</code></strong>, <strong><code>positions</code></strong>)</p>
</blockquote>
<p>Gathers the vectors at the specific positions over a minibatch.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

